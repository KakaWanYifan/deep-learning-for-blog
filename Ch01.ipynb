{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "def loss(b,w,xArr,yArr):\n",
    "    '''\n",
    "    损失函数\n",
    "    :param b: 偏置\n",
    "    :param w: 权重\n",
    "    :param xArr: xArr\n",
    "    :param yArr: yArr\n",
    "    :return: 损失函数的值\n",
    "    '''\n",
    "    # 损失值\n",
    "    total_loss = 0\n",
    "    for i in range(0,len(xArr)):\n",
    "        x = xArr[i]\n",
    "        y = yArr[i]\n",
    "        total_loss = total_loss + (y - (w * x + b)) ** 2\n",
    "\n",
    "    return total_loss/(float(len(xArr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [],
   "source": [
    "# 一步，梯度\n",
    "def step_gradient(b_cur,w_cur,xArr,yArr,lr):\n",
    "    '''\n",
    "    一步，梯度\n",
    "    :param b_cur: 当前的偏置\n",
    "    :param w_cur: 当前的权重\n",
    "    :param xArr: xArr\n",
    "    :param yArr: yArr\n",
    "    :param lr: 学习率\n",
    "    :return:\n",
    "    '''\n",
    "    b_gradient = 0\n",
    "    w_gradient = 0\n",
    "    n = float(len(xArr))\n",
    "    for i in range(0,len(xArr)):\n",
    "        x = xArr[i]\n",
    "        y = yArr[i]\n",
    "        w_gradient = w_gradient + (2/n) * ((w_cur * x + b_cur) - y) * x\n",
    "        b_gradient = b_gradient + (2/n) * ((w_cur * x + b_cur) - y)\n",
    "    b_new = b_cur - (lr * b_gradient)\n",
    "    w_new = w_cur - (lr * w_gradient)\n",
    "\n",
    "    return b_new,w_new\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 迭代更新\n",
    "def gradient_descent(b_start,w_start,xArr,yArr,lr,iterations):\n",
    "    '''\n",
    "    迭代更新\n",
    "    :param points: 数据点\n",
    "    :param b_start: 偏置的初始值\n",
    "    :param w_start: 权重的初始值\n",
    "    :param lr: 学习率\n",
    "    :param iterations: 最大迭代次数\n",
    "    :return:\n",
    "    '''\n",
    "    b = b_start\n",
    "    w = w_start\n",
    "    for i in range(iterations):\n",
    "        b,w = step_gradient(b_cur=b,w_cur=w,xArr=xArr,yArr=yArr,lr=lr)\n",
    "    return b,w"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初值的损失： 136.91315134241083\n",
      "b： 1.0170737051230807    w： 1.9987051102211508\n",
      "训练后的损失： 0.027859262495676634\n"
     ]
    }
   ],
   "source": [
    "# 等差数列\n",
    "xArr = np.linspace(-10,10,100)\n",
    "# 加上噪音\n",
    "yArr = 2.0 * xArr + 1.0 + np.random.randn(len(xArr)) * 0.2\n",
    "\n",
    "\n",
    "# 给b和w赋初值\n",
    "b = 0\n",
    "w = 0\n",
    "\n",
    "# 初值的损失\n",
    "print('初值的损失：',loss(b=b,w=w,xArr=xArr,yArr=yArr))\n",
    "\n",
    "# 迭代更新 学习率0.001,迭代10000次\n",
    "b,w = gradient_descent(b_start=b,w_start=w,xArr=xArr,yArr=yArr,lr=0.001,iterations=100000)\n",
    "print('b：',b,'   w：',w)\n",
    "print('训练后的损失：',loss(b=b,w=w,xArr=xArr,yArr=yArr))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}